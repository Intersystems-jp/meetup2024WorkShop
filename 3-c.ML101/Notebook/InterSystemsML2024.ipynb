{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 第2回開発者コミュニティミートアップ 機械学習で手書き数字の識別に挑戦\n",
        "\n",
        "このワークショップでは、MNISTが提供している手書き数字の画像データセットを使用し、手書きの数字を識別する分類器の作成を通して、機械学習の基本をハンズオン形式で学びます。このワークショップの内容は、以下の書籍を参考にしています。\n",
        "\n",
        "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, Third Edition, O'Reilly Media Inc. 　\n",
        "<br> (邦訳)scikit-learn、Keras、TensorFlowによる実践機械学習 第2版"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 必要なライブラリをインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0nTq5Jp0CBE"
      },
      "source": [
        "## MNISTのデータセットをダウンロード\n",
        "ダウンロードされたデータを、変数mnistに格納しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n",
            "**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n",
            "**Please cite**:  \n",
            "\n",
            "The MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n",
            "\n",
            "It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n",
            "\n",
            "With some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n",
            "\n",
            "The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n",
            "\n",
            "Downloaded from openml.org.\n"
          ]
        }
      ],
      "source": [
        "print(mnist.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "glyB-8-2yAO0"
      },
      "outputs": [],
      "source": [
        "mnist = fetch_openml('mnist_784', as_frame=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtxKrk_30b7z"
      },
      "source": [
        "## データの内容の確認\n",
        "dataには、数字の画像データが入っています。データは、Numpyの2次元配列です。\n",
        "mnist.data.shapeには、データの「形」が入っています。(70000, 784)は、700000文字分のデータが入っていて、各文字（数字）は784次元の配列で表されていることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCQP8LHG0Rdt",
        "outputId": "0c206d90-2ff5-4684-fcd8-ed42ffd186b2"
      },
      "outputs": [],
      "source": [
        "mnist.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPS9rJOO1Zz9"
      },
      "source": [
        "### 数字イメージのデータを表示\n",
        "784要素は、28x28ピクセルのモノクロイメージを表しています。（白=0, 黒=255）\n",
        "mnist.data[]の添え字を変えて、いろいろなデータの中身を表示してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDA5hAxR1jCU",
        "outputId": "f50be707-4569-4e2f-f70f-69bc51053a02"
      },
      "outputs": [],
      "source": [
        "mnist.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### イメージを表示\n",
        "次のコードの mnist.data[]] の添字に、0~69999 の数字を入れて、いろいろなイメージを確認してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_digit(image_data):\n",
        "    image = image_data.reshape(28, 28)\n",
        "    plt.imshow(image, cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "some_digit = mnist.data[0]\n",
        "plot_digit(some_digit)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 正解データの確認\n",
        "mnist.targetは70000要素のNumpy 1次元配列で、mnist.dataの画像イメージが実際どの数字であるか（正解データ、ラベル）が格納されています。mnist.data[0]の正解がmnist.target[0],...,mnist.data[69999]の正解がmnist.target[69999]というふうに対応づけられています。\n",
        "\n",
        "例えば、mnist.data[55]のイメージの正解は、mnist.target[55]です（=8）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(mnist.target.shape)\n",
        "print(mnist.target)\n",
        "print(mnist.target[55])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 教師データの準備\n",
        "\n",
        "機械学習では、説明変数（ここでは画像イメージ）をX、目的変数（正解データ）をyとすることが多いので、X, yにデータを代入しておきます。\n",
        "\n",
        "この説明変数と目的変数のペアを教師データと呼びます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = mnist.data\n",
        "y = mnist.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## データの分割\n",
        "データを、学習用とテスト用に分割します。ここでは、70000個のデータのうち、学習用に60000個、テスト用に10000個とします。\n",
        "\n",
        "> *【Q】 データを学習用とテスト用に分割する理由はなんでしょう？*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2値分類器の学習\n",
        "0~9の数字のイメージを分類することは多値分類タスクです。数字が10種類あるので、10種類の中から一つの正解を選択するからです。\n",
        "\n",
        "しかし、ここでは、学習の仕組みを理解するために、数字のイメージが5か5でないかを分類する2値分類タスクを題材とし、学習にまつわる事項を取り上げていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まず、正解データを、5であればTrue、そうでなければFalseに変形し、y_train_5, y_test_5に格納します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_5 = (y_train == '5')\n",
        "y_test_5 = (y_test == '5')\n",
        "\n",
        "print(y_train_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SGD(確率的勾配降下法)を使って学習\n",
        "ここでは、学習アルゴリズムとして、SGD確率勾配降下法を使って学習を行います。\n",
        "\n",
        "SGD確率勾配降下法は、シンプルな線形分類器です。様々なパラメータが指定可能ですが、今回はデフォルト値を利用します。\n",
        "\n",
        "Scikit learnライブラリのSGDClassifierクラスを使用すれば、学習の詳細を知らなくても容易に学習が可能です。fit()メソッドに学習用データと、それに対応する正解データを渡すだけで学習が実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sgd_clf = SGDClassifier(random_state=42)\n",
        "sgd_clf.fit(X_train, y_train_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 予測と正解の比較\n",
        "\n",
        "分類器が5だと予測したデータの番号と、正解データの中の5のデータの番号を比較してみましょう。画面の都合上、最初の100個のデータで比較します。\n",
        "\n",
        "> *多くのデータで、予測と正解が一致しており、一部のデータでは一致していないことを確かめてみましょう。*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(np.where(sgd_clf.predict(X[0:1000])))\n",
        "print(np.where(y[0:1000]=='5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### モデルの評価\n",
        "モデルがどのくらい正確に予測を行っているかを定量的に把握することは重要です。機械学習の性能を測る指標には代表的なものがいくつかありますが、まずは、最もシンプルな指標である「精度(precision)」を算出します。精度は、予測が正解と一致したデータ数の全体に対する割合です。\n",
        "\n",
        "cross_val_score()は、クロスバリデーション(交差検証)と呼ばれる手法で、複数回学習とテストを繰り返す関数です。cv=3とすることで、繰り返しを3回にしています。\n",
        "\n",
        "くり返しの数だけ結果（精度）が返されます。\n",
        "\n",
        "> *【Q】 返された精度は高いと言えるでしょうか？*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ダミー分類器の性能\n",
        "95%を超える精度は感覚的に高いと思うかもしれません。しかし、実際には精度だけで性能を評価することは良い方法ではありません。それを示すために、ダミー分類器を使います。ダミー分類器は、正解データのうち最も頻度の高い値（今回はFalse: 5ではない）を返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dummy_clf = DummyClassifier()\n",
        "dummy_clf.fit(X_train, y_train_5)\n",
        "print(any(dummy_clf.predict(X_train)))  # prints False: no 5s detected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ダミー分類器でもクロスバリデーションで精度を測定してみます。\n",
        "\n",
        "> 　*【Q】得られた精度について、なぜこのような（高い）値が出るのか考えてみましょう。*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 混同行列を使った性能評価\n",
        "\n",
        "予測が当たっているかどうかに着目する指標が精度です。しかし、ダミー分類器のように常にFalseと予測しても、正解がほとんどFalseであれば、高い精度が出てしまうのが問題です。\n",
        "\n",
        "予測の当たり外れを詳細に考えてみると、予測が外れる時に、Trueが正解であるものにFalseと予測する場合と、Falseが正解である場合にTrueと予測する場合があることに気づきます。このような状況を整理するのに、混同行列が役に立ちます。\n",
        "\n",
        "混同行列の説明は、[README.mdの説明](https://github.com/Intersystems-jp/meetup2024WorkShop/blob/main/3-c.ML101/README.md#混同行列)を見てください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_train_5, y_train_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(y_train_5, y_train_pred, output_dict=True)).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_pred_dummy = cross_val_predict(dummy_clf, X_train, y_train_5, cv=3)\n",
        "cm = confusion_matrix(y_train_5, y_train_pred_dummy)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(y_train_5, y_train_pred_dummy, output_dict=True)).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sgd_clf.decision_function(X_train[0:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
        "                             method=\"decision_function\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0\n",
        "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
        "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
        "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
        "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
        "plt.axis([-50000, 50000, 0, 1])\n",
        "plt.grid()\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_pred = sgd_clf.predict(X_test)\n",
        "pd.DataFrame(classification_report(y_test_5, y_test_pred, output_dict=True)).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "svm_clf = SVC(random_state=42)\n",
        "svm_clf.fit(X_train, y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(svm_clf.predict(X[0:50]))\n",
        "print(y[0:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forest_clf = RandomForestClassifier(random_state=42)\n",
        "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
        "cm = confusion_matrix(y_train_5, y_train_pred_forest)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(y_train_5, y_train_pred_forest, output_dict=True)).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forest_clf.fit(X_train, y_train_5)\n",
        "y_test_pred_forest = forest_clf.predict(X_test)\n",
        "pd.DataFrame(classification_report(y_test_5, y_test_pred_forest, output_dict=True)).transpose()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IRIS Python",
      "language": "python",
      "name": "irispython"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
